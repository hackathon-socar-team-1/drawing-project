# -*- coding: utf-8 -*-
"""crawl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QdOf3xy28xQxvAN2d2rm6g6LODYcY9AL
"""

# !pip install beautifulsoup4
# !pip install selenium
# !pip install webdriver-manager
#
# !pip install selenium
# !apt-get update
# !apt install chromium-chromedriver
#
# !pip install bs4
#
# from google.colab import drive
# drive.mount('/content/drive')

import os
from selenium import webdriver
from urllib.parse import quote_plus
from urllib.request import urlopen
import os
from selenium.common.exceptions import NoSuchElementException


from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
root = os.path.join(os.getcwd(), "crawl_img")
s = Service('C:\\Users\\qorgh2akfl\\Desktop\\chromedriver_win32(1)\\chromedriver.exe')
root

import pandas as pd
import requests
import requests
from urllib.request import urlopen
from urllib.parse import quote_plus
from bs4 import BeautifulSoup 
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.keys import Keys
import time
import urllib.request
import os
from selenium.webdriver.common.by import By
import cv2
from urllib import request
import time
from selenium.webdriver.chrome.service import Service
#s = Service('C:\\Users\\qorgh2akfl\\Desktop\\chromedriver_win32\\chromedriver.exe')
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
#driver = webdriver.Chrome(service=s, options=chrome_options)
driver = webdriver.Chrome(ChromeDriverManager().install())

url_list = [] # 클릭할 상세 장소 url
place_list = [] # 장소 이름
tel_list = [] # 전화번호
address_list = [] # 주소
longitude_list = [] # 경도
latitude_list = [] # 위도
image = []
text_file = open('./text/save.txt', 'r', encoding="UTF-8")
x = text_file.read()
x = str(x)
print(x)
searchtext = x+" "+"가볼만한곳"
nmap_url = 'https://m.map.naver.com/search2/search.naver?query='+searchtext
driver.get(nmap_url)
time.sleep(5)
req = driver.page_source
soup = BeautifulSoup(req, 'html.parser')

# headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36"}
# res = requests.get(nmap_url, headers=headers)
# res.raise_for_status()
# soup = BeautifulSoup(res.text, "lxml") 
#driver = webdriver.Chrome(service=s)

#ct > div.search_listview._content._ctList > ul > li:nth-child(3) > div.item_info > a.item_thumb._itemThumb > img
#ct > div.search_listview._content._ctList > ul

# thumbnails = soup.select("#ct > div.search_listview._content._ctList > ul > li:nth-of-type(1) > div.item_info > a.item_thumb._itemThumb > img")
# print(thumbnails)
# for thumb in thumbnails:
# 	src = thumb["class"]
# 	print(src)

# product_img_url = soup.find("img")["src"]
# print(product_img_url)
#product_img_url = "https:"+soup.select_one('#ct > div.search_listview._content._ctList > ul > li:nth-of-type(3) > div.item_info > a.item_thumb._itemThumb > img')['src']
for i in range(1,10):
    xpath = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-sid")
    url_list.append('https://m.place.naver.com/restaurant/'+xpath+'/home')
    xpath_title = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-title")
    place_list.append(xpath_title)
    xpath_tel = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-tel")
    tel_list.append(xpath_tel)
    xpath_address = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']/div[1]/div[1]/div/a').text
    address_list.append(xpath_address[5:])
    xpath_longitude = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-longitude")
    longitude_list.append(xpath_longitude)
    xpath_latitude = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-latitude")
    latitude_list.append(xpath_latitude)
    try:
        pre = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']/div[1]/a[1]/img')
        xpath_image = driver.find_element(By.XPATH, '/html/body/div[4]/div[2]/ul/li[' + str(
            i) + ']/div[1]/a[1]/img').get_attribute("src")
        if (xpath_image == None):
            xpath_image = driver.find_element(By.XPATH, '/html/body/div[4]/div[2]/ul/li[' + str(
                i) + ']/div[1]/a[1]/img').get_attribute("data-url")
        urllib.request.urlretrieve(xpath_image, root + "/" + xpath_title + ".png")
        image.append(xpath_image)
    except NoSuchElementException:
        print("No element found")
        xpath_image = "no image"
        image.append(xpath_image)






          #print(xpath_image)

    # if (xpath_image != "None"):


    # name = os.path.join(root,str(i)+".jpg")
    # print(name)
    # cv2.imwrite("root/k.jpg",xpath_image)

    # imgs = driver.find_elements_by_css_selector('img._thumbImg')
    # for img in imgs:
    #   print(img.get_attribute('src')) 
    # xpath_image = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li['+str(i)+']').get_attribute("data-latitude")
# xpath_image2 = driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/ul/li[5]/div[1]/a[1]').get_attribute("data-rank")
# print(url_list, place_list, tel_list, address_list, latitude_list, longitude_list,image)
df = pd.DataFrame({'link' : url_list, 'name' : place_list, 'tel' : tel_list, 'address' : address_list,
                    'latitude' : latitude_list, 'longitude' : longitude_list,'image':image})
# print(product_img_url)

df.head(5)
df.to_csv("df.csv")
image

from bs4 import BeautifulSoup
from selenium import webdriver
import time

# driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)
# driver.get('https://m.map.naver.com/search2/search.naver?query=가볼만한곳')
# time.sleep(5) # 5초 동안 페이지 로딩 기다리기
#
# req = driver.page_source
# soup = BeautifulSoup(req, 'html.parser')
# image = []
# for i in range(1,10):
#   thumbnails = soup.select("#ct > div.search_listview._content._ctList > ul > li:nth-of-type(3) > div.item_info > a.item_thumb._itemThumb > img")
#   print(thumbnails)
#   # for thumb in thumbnails:
#   #   src = thumb["src"]
#   #   image.append(src)
#
# print(1)
# print(image)
# driver.quit() # 끝나면 닫아주기